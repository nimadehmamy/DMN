{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "args = \"{'nf1': 6, 's':2, 'nf2':40, 'nb_epoch': 50}\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "#args = (sys.argv[1] if len(sys.argv)>1 else \"{}\")\n",
    "\n",
    "def params(nf1 = 9, s = 0, nf2 = 81, nb_epoch = 100, batch_size = 128):\n",
    "    return nf1, s, nf2, nb_epoch, batch_size\n",
    "nf1, s, nf2, nb_epoch, batch_size = params(**eval(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from dmn2 import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from numpy.linalg import eig, eigh\n",
    "import numpy as np\n",
    "from numpy.random import rand, randint, randn\n",
    "from scipy.signal import convolve2d\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import time\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "class layer():\n",
    "    rho = 0\n",
    "    num_im = 0  # to help w/ normalization \n",
    "    def __init__(self,data, n='auto', k= 3, nf='all', stride = 1, out=True, **kw):\n",
    "        self.k = k\n",
    "        self.stride = stride\n",
    "        print \"making density matrix...\"\n",
    "        #self.get_rho(data)\n",
    "        self.get_rho_fast(data)\n",
    "        print \"density matrix done!\"\n",
    "        self.in_channels = (data[0].shape[0] if len(data[0].shape) ==3 else 1)\n",
    "        nmax = self.in_channels*self.k**2\n",
    "        if (nf == 'all') or (nf >= nmax):\n",
    "            self.num_filters = self.rho.shape[0]\n",
    "            self.get_eigs0()\n",
    "        else:\n",
    "            self.num_filters = min(nf, nmax-1) \n",
    "            self.get_eigs()\n",
    "        \n",
    "        self.__setups()\n",
    "        \n",
    "    def setups(self):\n",
    "        self.output =0 \n",
    "        self.setup_output()\n",
    "        self.setup_normalized_output()\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "        \n",
    "    def setup_output(self):\n",
    "        self.ims = tf.placeholder(np.float32)\n",
    "        self.flt = tf.placeholder(np.float32)\n",
    "        self.output = tf.nn.conv2d(self.ims, self.flt, \n",
    "                        strides= [1,1,1,1],padding= \"VALID\")\n",
    "        \n",
    "    def setup_normalized_output(self):\n",
    "        #x = tf.placeholder(dtype=tf.float32)\n",
    "        mn, var = tf.nn.moments(self.ims,axes=[2,3],keep_dims=True)\n",
    "        self.normalized_output = (self.ims-mn)/tf.sqrt(var)\n",
    "        \n",
    "    def get_output(self,ims,idx = 'all', batch = 5000):\n",
    "        if idx == 'all':\n",
    "            fls = self.filters\n",
    "        else:\n",
    "            fls = self.filters[idx]\n",
    "\n",
    "        s1 = self.filters.shape\n",
    "        k = self.k\n",
    "        n = np.prod(s1)/s1[0]/k**2\n",
    "        # to shape [out_chan, in_chan, x,y]\n",
    "        f1 = fls.reshape(s1[0],n, k, k)\n",
    "        # transpose to [x,y,in, out]\n",
    "        f1 = f1.transpose((2,3,1,0)).astype(np.float32)\n",
    "        out = []\n",
    "        for i in range(0,len(ims), batch):\n",
    "            im = np.array(ims[i:batch+i],dtype= np.float32).transpose((0,2,3,1)) \n",
    "            out += [self.output.eval({self.flt: f1, self.ims: im}).transpose((0,3,1,2))]\n",
    "        return np.concatenate(out)\n",
    "    \n",
    "    def get_normalized_output(self,ims):\n",
    "        #return self.normalized_output.eval({self.ims: ims})\n",
    "        mn = ims.mean(axis=(1,2),keepdims=True)\n",
    "        st = ims.std(axis=(1,2),keepdims=True)\n",
    "        return (ims - mn)/st \n",
    "    \n",
    "    def inc_rho(self, b):\n",
    "        n = self.num_im\n",
    "        self.rho = (n * self.rho + b)/(n+1.)\n",
    "        self.num_im += 1\n",
    "    \n",
    "    def sub_rho(self,im):\n",
    "        \"\"\"NOTE: This function uses a fixed grid on the image to calculate rho \n",
    "        i.e. the stride for sub ims is k, same as sub im size. \n",
    "        To get fully accurate rho, stride 1 should be used, but it really won't matter in real images.\"\"\"\n",
    "        k = self.k\n",
    "        s = np.array(im.shape)\n",
    "        ch = s[0] # channels\n",
    "        sx,sy = s[1:]/k\n",
    "        # reshape to get sub ims\n",
    "        p1 = im[:,:k*sx,:k*sy].reshape((ch,sx,k,sy,k)).transpose((1,3,0,2,4)) # now p[i,j] are sub-ims\n",
    "        p1 = p1.reshape((sx*sy,ch*k*k))\n",
    "        # ! memory intensive, but fast...\n",
    "        #rho = (p1[:,np.newaxis]*p1[:,:,np.newaxis]).mean(0)\n",
    "        rho = 0\n",
    "        for p in p1:\n",
    "            rho = rho + p*p[:,np.newaxis]\n",
    "        return rho/p1.shape[0]\n",
    "    \n",
    "    def get_rho_fast (self, data):\n",
    "        print \"rho: processing images:\",\n",
    "        for im in data:\n",
    "            #print im.shape\n",
    "            for ki in range(0,self.k-1,self.stride):\n",
    "                for kj in range(0,self.k-1,self.stride):\n",
    "                    self.inc_rho( self.sub_rho(im[:,ki:,kj:]))\n",
    "            \n",
    "    def get_eigs0(self):\n",
    "        self.eigs = eig(self.rho)\n",
    "        # sort eigs from  largest to smallest  \n",
    "        #idx = argsort(real(self.eigs[0]))[::-1]\n",
    "        self.energies = -real(log(self.eigs[0]/self.eigs[0].sum()))\n",
    "        idx = argsort(self.energies)\n",
    "        self.energies = self.energies[idx]\n",
    "        ### NOTE! filters are rows of self.filters, not columns like in eigs!!\n",
    "        self.filters = self.eigs[1][:,idx].T  \n",
    "        \n",
    "    def get_eigs(self):\n",
    "        self.eigs = TruncatedSVD(n_components=self.num_filters, n_iter=7, random_state=42)\n",
    "        self.eigs.fit(self.rho) # \n",
    "        self.energies = -real(log(self.eigs.explained_variance_/self.eigs.explained_variance_.sum()))\n",
    "        idx = argsort(self.energies)\n",
    "        self.energies = self.energies[idx]\n",
    "        ### NOTE! filters are rows of self.filters, not columns like in eigs!!\n",
    "        self.filters = self.eigs.components_[idx]\n",
    "        \n",
    "    def truncate(self, nf):\n",
    "        self.energies = self.energies[:nf]\n",
    "        self.filters = self.filters[:nf]\n",
    "        #\n",
    "    def update_layer(self,data):\n",
    "        self.get_rho_fast(data)\n",
    "        self.get_eigs()\n",
    "        \n",
    "    \n",
    "    def get_filter_outputs(self,im,idx = 'all'):\n",
    "        \"\"\"\n",
    "        apply filters to an image `im`. If idx = [ni,... nf] given, only output of filters[idx] is returned\n",
    "        Output: ndarray (images x filters)  \n",
    "        \"\"\"\n",
    "        if idx =='all':\n",
    "            eigvec = self.filters\n",
    "        else: eigvec = self.filters[idx]\n",
    "        #print \"eig:\", eigvec.shape, self.k, im.shape\n",
    "        if len(im.shape) == 2:\n",
    "            return np.array([convolve2d(im,i.reshape(self.k,self.k), mode='valid') for i in eigvec])\n",
    "        else:\n",
    "            nf0 = im.shape[0]\n",
    "            k2 = self.k**2\n",
    "            out = []\n",
    "            for i in eigvec:\n",
    "                # each row is the output of one filter from previous layer\n",
    "                # It should be convolved with the corresponding rows in each eigvec\n",
    "                out += [np.array([convolve2d(imf,i[ii*k2:(ii+1)*k2].reshape(self.k,self.k),\n",
    "                                         mode='valid') for imf, ii in zip(im, range(nf0))]).sum(0)]\n",
    "                # to sum over nf0 channels\n",
    "\n",
    "            return np.array(out) # shape (nf1, x-k, y-k)\n",
    "    \n",
    "    def get_output_old(self,images):\n",
    "        ii = 0\n",
    "        out = []\n",
    "        for im in images:\n",
    "            #print ii,\n",
    "            out += [self.get_filter_outputs(im)]\n",
    "            ii+=1 \n",
    "        return out\n",
    "\n",
    "        \n",
    "    ### Vizualization\n",
    "    def viz_filters(self, n='all'):\n",
    "        k = self.k\n",
    "        if n=='all':\n",
    "            F,E = self.filters, self.energies\n",
    "        else:\n",
    "            F,E = self.filters[n], self.energies[n] \n",
    "        for c in range(self.in_channels):\n",
    "            figure(figsize=(8,8))\n",
    "            ii = 1\n",
    "            print \"For Input filter %d\" %c\n",
    "            k1 = int(sqrt(len(F)))+1\n",
    "            for i,en in zip(F, E):\n",
    "                subplot(k1,k1,ii)\n",
    "                title('%.3g'%en)\n",
    "                ii+=1\n",
    "                imshow(real(i[c*k**2:(c+1)*k**2].reshape(k,k)),cmap='binary')\n",
    "                xticks([])\n",
    "                yticks([])\n",
    "            show()\n",
    "            \n",
    "    \n",
    "            \n",
    "\n",
    "class DMN():\n",
    "    def __init__(self,ims, labels ):\n",
    "        \"\"\"Create a Density Matrix Network ;)\n",
    "        It will initialize with no layers. \n",
    "        you generate layers by invoking the self.create_layer() method, \n",
    "        creating an instance of the `layer` class.  \n",
    "        ims: set of input images\n",
    "        \"\"\"\n",
    "        # print \"Adding random noise to avoid high degeneracies...\"\n",
    "        self.labels = labels\n",
    "        if len(ims[0].shape)==3:\n",
    "            self.output = array([(i+rand(*i.shape)).transpose((2,0,1)) for i in ims])\n",
    "        else:\n",
    "            self.output = array([(i+rand(*i.shape))[np.newaxis,:] for i in ims])\n",
    "        self.layers = []\n",
    "        self.__setup_pooling()\n",
    "        \n",
    "    def __setup_pooling(self, k = 2):\n",
    "        self.out_ = tf.placeholder(dtype=tf.float32)\n",
    "        self.pool_k_ = tf.placeholder(dtype=tf.int8)\n",
    "        self._max_pool = tf.nn.max_pool(self.out_, \n",
    "                                     ksize=[1,k,k,1], \n",
    "                                     strides= [1,k,k,1],\n",
    "                                     padding= \"SAME\")#, data_format=\"NCHW\")\n",
    "        self._avg_pool = tf.nn.avg_pool(self.out_, \n",
    "                                     ksize=[1,k,k,1], \n",
    "                                     strides= [1,k,k,1],\n",
    "                                     padding= \"SAME\")#, data_format=\"NCHW\")\n",
    "    def pooling(self, data, type = 'max'):\n",
    "        assert net.output.dtype == np.float32\n",
    "        if type == 'max':\n",
    "            return self._max_pool.eval({self.out_: data.transpose(0,2,3,1) }).transpose(0,3,1,2)\n",
    "        elif type == 'avg':\n",
    "            return self._avg_pool.eval({self.out_: data.transpose(0,2,3,1) }).transpose(0,3,1,2)\n",
    "        \n",
    "    def create_layer(self,**kw):\n",
    "        print \"Propagating through last layer\"\n",
    "        if len(self.layers)>0:\n",
    "            pass # self.output = self.layers[-1].get_output(self.output)\n",
    "        self.layers +=[layer(self.output, **kw)]\n",
    "        \n",
    "    def get_filter(self,l1, l2,n1, n2):\n",
    "        ei_sq = l2.filters[n2].reshape((len(l1.filters),l2.k, l2.k))\n",
    "        return convolve2d(ei_sq[0], l1.filters[n1].reshape((l1.k,l1.k)))\n",
    "    \n",
    "    def get_mask(self,a, nlay):\n",
    "        l = self.layers[nlay-1]\n",
    "        n = len(l.filters)\n",
    "        c = l.filters.shape[1]/(l.k**2)\n",
    "        if len(a.shape)==1:\n",
    "            k = self.layers[nlay].k\n",
    "            a = a.reshape((n,k, k))\n",
    "        if nlay ==1: # second layer\n",
    "            sh = (c,l.k,l.k)\n",
    "            \n",
    "        else:\n",
    "            sh = (self.layers[nlay-2].filters.shape[0],l.k,l.k)\n",
    "        \n",
    "        # each filter in nlay-1 connects to nlay-2 filters, \n",
    "        # conv2d must yield #nlay-2 outputs \n",
    "        out =[0]*sh[0] \n",
    "        for i in range(n):\n",
    "            lf = l.filters[i].reshape(sh)\n",
    "            for j in range(sh[0]):\n",
    "                out[j] += convolve2d(a[i], lf[j])\n",
    "                \n",
    "        return np.array(out)\n",
    "    \n",
    "    def image_filter(self, nlay, nfil):\n",
    "        \"\"\"recursively go down the network and generate the mask on image. \n",
    "        !!! Needs to account for pooling layers\"\"\"\n",
    "        out = self.layers[nlay].filters[nfil]\n",
    "        for n in range(nlay)[::-1]:\n",
    "            #print n,\n",
    "            out = self.get_mask(out, n+1)\n",
    "        return out\n",
    "        \n",
    "    def max_pooling(self, size = 2):\n",
    "        # 1) get output of previous layer\n",
    "        # 2) downsample using max   \n",
    "        out = []\n",
    "        for fim in self.output: \n",
    "            filter_out = [] # for each filter, we maxpool the output\n",
    "            for im in fim:\n",
    "                s = im.shape\n",
    "                ds = np.array([[im[i*size:(i+1)*size,j*size:(j+1)*size].max() \\\n",
    "                            for j in range(s[1]/size)] for i in range(s[0]/size)])\n",
    "                filter_out+=[ds]\n",
    "            out +=[np.array(filter_out)]\n",
    "        self.output = np.array(out)\n",
    "\n",
    "\n",
    "def get_mask(self,a, nlay):\n",
    "        l = self.layers[nlay-1]\n",
    "        n = len(l.filters)\n",
    "        c = l.filters.shape[1]/(l.k**2)\n",
    "        if len(a.shape)==1:\n",
    "            k = self.layers[nlay].k\n",
    "            a = a.reshape((n,k, k))\n",
    "        if nlay ==1: # second layer\n",
    "            sh = (c,l.k,l.k)\n",
    "            \n",
    "        else:\n",
    "            sh = (self.layers[nlay-2].filters.shape[0],l.k,l.k)\n",
    "        \n",
    "        # each filter in nlay-1 connects to nlay-2 filters, \n",
    "        # conv2d must yield #nlay-2 outputs \n",
    "        out =[0]*sh[0] \n",
    "        for i in range(n):\n",
    "            lf = l.filters[i].reshape(sh)\n",
    "            for j in range(sh[0]):\n",
    "                out[j] += convolve2d(a[i], lf[j])\n",
    "                \n",
    "        return np.array(out)\n",
    "    \n",
    "class tictoc():\n",
    "    prev = 0\n",
    "    current = 0\n",
    "    def tic(self):\n",
    "        self.current = time.time()\n",
    "        \n",
    "    def toc(self):\n",
    "        self.prev = self.current + 0.\n",
    "        self.current = time.time()\n",
    "        print \"time:\", self.current-self.prev\n",
    "        \n",
    "tc= tictoc()\n",
    "\n",
    "def get_ims(fnam = '/home/ivplroot/Downloads/DeepLearning/dataset/mnist_test.csv'):\n",
    "    f = open(fnam, 'r')\n",
    "    a = [np.int0(i.split(',')) for i in f.readlines()]\n",
    "    f.close()\n",
    "    label = [i[0] for i in a]\n",
    "    im_arr = [i[1:].reshape((28,28)) for i in a] # \n",
    "    return np.array(label), im_arr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from numpy import *\n",
    "# import numpy as np\n",
    "# rand = np.random.rand\n",
    "# eig = np.linalg.eig\n",
    "# eigh = np.linalg.eigh\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from keras import callbacks\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "#get_ipython().magic(u'pylab inline')\n",
    "import os,time\n",
    "from scipy.io import loadmat\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "class layer2(layer):\n",
    "    def __init__(self,data,labels, n='auto', k= 3, nf='all', \n",
    "                 stride = 1, out=True, th_eig=0.95 ,th_corr=.9, **kw):\n",
    "        self.rho = 0\n",
    "        self.num_im = 0  # to help w/ normalization \n",
    "        self.k = k\n",
    "        self.th_eig = th_eig\n",
    "        self.th_corr = th_corr\n",
    "        self.stride = stride\n",
    "        print \"making density matrix...\"\n",
    "        self.get_rhos(array(data), labels)\n",
    "        self.get_eigs()\n",
    "        self.in_channels = (data[0].shape[0] if len(data[0].shape) ==3 else 1)\n",
    "        self.truncate()\n",
    "         \n",
    "        self.setups()\n",
    "        \n",
    " \n",
    "    def get_rhos(self, data, labels):\n",
    "        self.rhos = {}\n",
    "        print \"rho_c: \", \n",
    "        for s in set(labels):\n",
    "            print s, \n",
    "            idx = where(labels==s)\n",
    "            self.rho = 0\n",
    "            print data[idx].shape\n",
    "            self.get_rho_fast(data[idx])\n",
    "            \n",
    "            self.rhos[int(s)] = self.rho\n",
    "        \n",
    "    def get_eigs(self):\n",
    "        self.eigs = {}\n",
    "        for l,rs in self.rhos.iteritems():\n",
    "            ei = eigh(rs)\n",
    "            idx = argsort(-ei[0])\n",
    "            fil = ei[1][:,idx]\n",
    "            eng = (ei[0]/ei[0].sum())[idx]\n",
    "#             eng = np.array(sorted(ei[0] / ei[0].sum()))[::-1]\n",
    "            ix = where(eng.cumsum() <= self.th_eig)[0]\n",
    "            \n",
    "            self.eigs[l] = (eng[ix], fil[:,ix])\n",
    "        \n",
    "    def truncate(self):\n",
    "        #fil_flat = np.array(9*[[]])\n",
    "        i = 0\n",
    "        for k in sorted(self.eigs):\n",
    "            v = self.eigs[k] \n",
    "            s = v[1].shape\n",
    "            eng = v[0]# / v[0].sum()\n",
    "            fil = v[1].T#.reshape((s[0]*s[1], s[2]))\n",
    "            if i==0:\n",
    "                i+=1\n",
    "                eng_flat = eng\n",
    "                fil_flat = fil\n",
    "            else:\n",
    "                eng_flat = concatenate([eng_flat, eng])\n",
    "                fil_flat = concatenate([fil_flat, fil],axis=0)\n",
    "        self.cc = corrcoef(fil_flat)\n",
    "        \n",
    "        l = range(len(fil_flat))\n",
    "        for i in l:\n",
    "            ix = where(abs(self.cc[i])>self.th_corr)[0]\n",
    "            for j in ix:\n",
    "                if j!=i: \n",
    "                    try: \n",
    "                        l.remove(j)\n",
    "                        print '(%d, %.2f)'%(j,self.cc[i,j]),\n",
    "                    except ValueError: \n",
    "                        pass\n",
    "        \n",
    "        self.energies = eng_flat[l]\n",
    "        self.filters = fil_flat[l]\n",
    "        #\n",
    "class DMN2(DMN):\n",
    "    \n",
    "    def create_layer(self, frac = 1., **kw):\n",
    "        \"\"\" frac: fraction of input and labels to use for training\n",
    "        n='auto', k= 3, nf='all', stride = 1, out=True, \"\"\"\n",
    "        idx = argsort(rand(len(self.labels)))[:int(frac*len(self.labels))] \n",
    "        if len(self.layers)>0:\n",
    "            pass # self.output = self.layers[-1].get_output(self.output)\n",
    "        self.layers +=[layer2(self.output[idx],self.labels[idx], **kw)]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = 10\n",
    "\n",
    "tc.tic()\n",
    "lab_test, pic_test = get_ims()\n",
    "lab_train, pic_train = get_ims('/home/ivplroot/Downloads/DeepLearning/dataset/mnist_train.csv')\n",
    "tc.toc()\n",
    "X_train, y_train = np.array(pic_train,dtype = np.float32).reshape(len(pic_train),1,28,28)/255., lab_train\n",
    "X_test, y_test = np.array(pic_test,dtype = np.float32).reshape(len(pic_test),1,28,28)/255., lab_test\n",
    "tc.toc()\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_test = np_utils.to_categorical(y_test, 10)\n",
    "tc.toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # 1DMN\n",
    "net = DMN2(pic_train, lab_train)\n",
    "net_test = DMN2(pic_test, lab_test)\n",
    "\n",
    "tc.tic()\n",
    "net.create_layer(frac = .3,k=3,out=1, th_eig = .95)\n",
    "tc.toc()\n",
    "l0 = net.layers[0]\n",
    "net.output = l0.get_output(net.output)\n",
    "tc.toc()\n",
    "X_train2 = l0.get_normalized_output(net.output)\n",
    "tc.toc()\n",
    "net_test.output = l0.get_output(net_test.output)\n",
    "tc.toc()\n",
    "X_test2 = l0.get_normalized_output(net_test.output)\n",
    "tc.toc()\n",
    "s = X_train2[0].shape\n",
    "num, m = s[0], s[-1]\n",
    "print \"# of filters: \", l0.filters.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2,2),input_shape=X_train2[0].shape, data_format = 'channels_first'))\n",
    "#model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, \n",
    "#          center=True, scale=True, beta_initializer='zeros', \n",
    "#        gamma_initializer='ones', moving_mean_initializer='zeros', #moving_variance_initializer='ones', \n",
    "#beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None))\n",
    "\n",
    "#model.add(Convolution2D(40, (3, 3), activation='relu', data_format = 'channels_first'))\n",
    "#model.add(MaxPooling2D(pool_size=(2,2), data_format = 'channels_first'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='softmax'))\n",
    " \n",
    "# 8. Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "filepath='../mnist_model/'+\"weights-improvement-{epoch:02d}-{acc:.4f}-{val_acc:.4f}-dmn%d-dmn%d-de10-iter%d.hdf5\"\\\n",
    "%(nf1, num, nb_epoch)\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "\n",
    "history = model.fit(X_train2, Y_train, batch_size=batch_size, epochs=nb_epoch,#callbacks=[earlyStopping],\n",
    "       callbacks=callbacks_list, verbose=0, validation_data=(X_test2, Y_test))\n",
    "\n",
    "# import cPickle as pk\n",
    "# pk.dump(history.history, open('../history/history-mnist-dmn%d-dmn%d_%d-d10-iter%d.pkl'%(nf1, s, num, nb_epoch), 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tc.tic()\n",
    "print net.output.shape\n",
    "net.output = net.pooling(net.output)\n",
    "net_test.output = net.pooling(net_test.output)\n",
    "tc.toc()\n",
    "net.create_layer(frac = .3,k=3,out=1, th_eig = .99)\n",
    "tc.toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tc.tic()\n",
    "l1 = net.layers[1]\n",
    "net.output = l1.get_output(net.output)\n",
    "tc.toc()\n",
    "X_train2 = l1.get_normalized_output(net.output)\n",
    "tc.toc()\n",
    "net_test.output = l1.get_output(net_test.output)\n",
    "tc.toc()\n",
    "X_test2 = l1.get_normalized_output(net_test.output)\n",
    "tc.toc()\n",
    "s = X_train2[0].shape\n",
    "num, m = s[0], s[-1]\n",
    "print \"# of filters: \", l1.filters.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l1.filters.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2,2),input_shape=X_train2[0].shape, data_format = 'channels_first'))\n",
    "#model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, \n",
    "#          center=True, scale=True, beta_initializer='zeros', \n",
    "#        gamma_initializer='ones', moving_mean_initializer='zeros', #moving_variance_initializer='ones', \n",
    "#beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None))\n",
    "\n",
    "#model.add(Convolution2D(40, (3, 3), activation='relu', data_format = 'channels_first'))\n",
    "#model.add(MaxPooling2D(pool_size=(2,2), data_format = 'channels_first'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='softmax'))\n",
    " \n",
    "# 8. Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "filepath='../mnist_model/'+\"weights2-improvement-{epoch:02d}-{acc:.4f}-{val_acc:.4f}-dmn%d-dmn%d-de10-iter%d.hdf5\"\\\n",
    "%(nf1, num, nb_epoch)\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "\n",
    "history = model.fit(X_train2, Y_train, batch_size=batch_size, epochs=nb_epoch,#callbacks=[earlyStopping],\n",
    "       callbacks=callbacks_list, verbose=0, validation_data=(X_test2, Y_test))\n",
    "\n",
    "# import cPickle as pk\n",
    "# pk.dump(history.history, open('../history/history-mnist-dmn%d-dmn%d_%d-d10-iter%d.pkl'%(nf1, s, num, nb_epoch), 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
